"""Initial schema check after adding IP

Revision ID: f5b76ed1b9bd
Revises:
Create Date: 2025-04-29 13:04:39.696706

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
# Import func if needed for server defaults, though NOW() is standard SQL
# from sqlalchemy.sql import func
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f5b76ed1b9bd'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        'users',
        sa.Column('id', sa.Integer(), primary_key=True),
        sa.Column('email', sa.String(), nullable=False, unique=True),
        sa.Column('hashed_password', sa.String(), nullable=False),
        sa.Column('is_active', sa.Boolean(), nullable=False, server_default=sa.true()),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.text('now()')),
        # Add any other columns your app expects
    )
    op.create_table(
        'memory_snapshots',
        sa.Column('id', sa.Integer(), primary_key=True, index=True),
        sa.Column('user_id', sa.Integer(), sa.ForeignKey('users.id'), nullable=False, index=True),
        sa.Column('snapshot_data', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('codename', sa.String(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()')),
    )
    op.create_table('reflection_logs',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('timestamp', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('reflection_text', sa.Text(), nullable=False),
    sa.Column('snapshot_ref', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('analysis_metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_reflection_logs_id'), 'reflection_logs', ['id'], unique=False)
    op.create_index(op.f('ix_reflection_logs_user_id'), 'reflection_logs', ['user_id'], unique=False)
    op.create_table('task_footprints',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('task_id', sa.String(), nullable=False),
    sa.Column('event_type', sa.String(), nullable=False),
    sa.Column('timestamp', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('snapshot_ref', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.Column('event_metadata', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_task_footprints_id'), 'task_footprints', ['id'], unique=False)
    op.create_index(op.f('ix_task_footprints_task_id'), 'task_footprints', ['task_id'], unique=False)
    op.create_index(op.f('ix_task_footprints_user_id'), 'task_footprints', ['user_id'], unique=False)
    # op.drop_index('ix_reflection_event_logs_id', table_name='reflection_event_logs')
    # op.drop_index('ix_reflection_event_logs_linked_hta_node_id', table_name='reflection_event_logs')
    # op.drop_index('ix_reflection_event_logs_reflection_id', table_name='reflection_event_logs')
    # op.drop_index('ix_reflection_event_logs_timestamp', table_name='reflection_event_logs')
    # op.drop_table('reflection_event_logs')
    # op.drop_index('ix_task_event_logs_id', table_name='task_event_logs')
    # op.drop_index('ix_task_event_logs_linked_hta_node_id', table_name='task_event_logs')
    # op.drop_index('ix_task_event_logs_task_id', table_name='task_event_logs')
    # op.drop_index('ix_task_event_logs_timestamp', table_name='task_event_logs')
    # op.drop_table('task_event_logs')
    # op.alter_column('memory_snapshots', 'snapshot_data',
    #            existing_type=postgresql.JSON(astext_type=sa.Text()),
    #            type_=postgresql.JSONB(astext_type=sa.Text()),
    #            nullable=True)
    # op.alter_column('memory_snapshots', 'created_at',
    #            existing_type=postgresql.TIMESTAMP(),
    #            type_=sa.DateTime(timezone=True),
    #            nullable=False,
    #            server_default=sa.text('now()')) # Added server_default for consistency
    # op.drop_index('ix_memory_snapshots_codename', table_name='memory_snapshots')
    # op.create_foreign_key(None, 'memory_snapshots', 'users', ['user_id'], ['id'])
    # op.drop_column('memory_snapshots', 'updated_at')
    # op.alter_column('users', 'is_active',
    #            existing_type=sa.BOOLEAN(),
    #            nullable=False,
    #            server_default=sa.true()) # Added server_default for consistency
    # op.alter_column('users', 'created_at',
    #            existing_type=postgresql.TIMESTAMP(),
    #            type_=sa.DateTime(timezone=True),
    #            nullable=False,
    #            server_default=sa.text('now()')) # Added server_default for consistency
    # print("Updating NULL values in users.updated_at...") # Optional: for logging
    # op.execute(
    #     "UPDATE users SET updated_at = NOW() WHERE updated_at IS NULL"
    # )
    # print("Finished updating NULL values.") # Optional: for logging
    # op.alter_column('users', 'updated_at',
    #            existing_type=postgresql.TIMESTAMP(timezone=True), # Assuming this type is correct from models.py
    #            nullable=False,
    #            server_default=sa.text('now()'), # Added server_default for consistency
    #            existing_server_default=sa.text('now()')) # Added existing for onupdate
    # op.drop_column('users', 'full_name')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Note: Reversing the update for NULLs isn't straightforward,
    # as we don't know which ones were originally NULL.
    # The downgrade path might need manual adjustment if this migration
    # needs to be reversible in practice.
    op.add_column('users', sa.Column('full_name', sa.VARCHAR(), autoincrement=False, nullable=True))
    op.alter_column('users', 'updated_at',
               existing_type=postgresql.TIMESTAMP(timezone=True),
               nullable=True, # Revert nullable back to True
               server_default=None, # Remove server default if it was added
               existing_server_default=sa.text('now()')) # Keep existing if needed? Check model
    op.alter_column('users', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.alter_column('users', 'is_active',
               existing_type=sa.BOOLEAN(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.add_column('memory_snapshots', sa.Column('updated_at', postgresql.TIMESTAMP(), autoincrement=False, nullable=True))
    # Need the actual constraint name to drop it reliably
    # Replace 'fk_memory_snapshots_user_id_users' with the correct name if needed
    op.drop_constraint('fk_memory_snapshots_user_id_users', 'memory_snapshots', type_='foreignkey')
    op.create_index('ix_memory_snapshots_codename', 'memory_snapshots', ['codename'], unique=False)
    op.alter_column('memory_snapshots', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               nullable=True, # Revert nullable back to True
               server_default=None) # Remove server default
    op.alter_column('memory_snapshots', 'snapshot_data',
               existing_type=postgresql.JSONB(astext_type=sa.Text()),
               type_=postgresql.JSON(astext_type=sa.Text()),
               nullable=True) # Assuming it was nullable before? Check old model/DB state
    op.create_table('task_event_logs',
        # ... (rest of the downgrade definition, ensure it matches the old state) ...
        # This part is complex and depends heavily on the exact previous state
        # It's often safer to restore from backup than to rely on complex downgrades
        # Placeholder for brevity:
        sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original task_event_logs definition
        sa.PrimaryKeyConstraint('id', name='task_event_logs_pkey')
    )
    # Recreate indexes for task_event_logs
    op.create_table('reflection_event_logs',
        # Placeholder for brevity:
        sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
        # Add other columns from the original reflection_event_logs definition
        sa.PrimaryKeyConstraint('id', name='reflection_event_logs_pkey')
    )
    # Recreate indexes for reflection_event_logs
    op.drop_index(op.f('ix_task_footprints_user_id'), table_name='task_footprints')
    op.drop_index(op.f('ix_task_footprints_task_id'), table_name='task_footprints')
    op.drop_index(op.f('ix_task_footprints_id'), table_name='task_footprints')
    op.drop_table('task_footprints')
    op.drop_index(op.f('ix_reflection_logs_user_id'), table_name='reflection_logs')
    op.drop_index(op.f('ix_reflection_logs_id'), table_name='reflection_logs')
    op.drop_table('reflection_logs')
    # ### end Alembic commands ###

